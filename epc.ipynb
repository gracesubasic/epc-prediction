{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import shap\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.metrics import ConfusionMatrixDisplay, confusion_matrix\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tableone import TableOne\n",
    "from xgboost import XGBClassifier\n",
    "from ydata_profiling import ProfileReport"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initial exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "data = pd.read_csv(os.path.join(\"../data\", \"certificates.csv\"), low_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Columns in data: \", len(data.columns))\n",
    "print(\"Rows in data: \", len(data))\n",
    "# Check if the same lodgement appears more than once in data\n",
    "print(\"Unique LMK_KEY: \", data.LMK_KEY.nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create data profiling report on full dataset - takes a few min to run\n",
    "profile = ProfileReport(data)\n",
    "profile.to_file(\"full_dataset.html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check basic stats on target variable (more info in profiling report)\n",
    "data[\"CURRENT_ENERGY_RATING\"].value_counts(normalize=True).mul(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I did some initial research and found the assessment procedure for EPC ratings: https://www.gov.uk/guidance/standard-assessment-procedure. I used some of the information presented there and in the links as a guide for narrowing down columns and when considering which information would likely be known if the EPC rating itself is unknown\n",
    "\n",
    "# Remove columns that I know I won't use based on their context\n",
    "\n",
    "# Addresses and locations - we want the model to generalise to all locations\n",
    "# UPRN = Unique Property Reference Number\n",
    "location_columns = [\n",
    "    \"ADDRESS1\",\n",
    "    \"ADDRESS2\",\n",
    "    \"ADDRESS3\",\n",
    "    \"POSTCODE\",\n",
    "    \"LMK_KEY\",\n",
    "    \"BUILDING_REFERENCE_NUMBER\",\n",
    "    \"LOCAL_AUTHORITY\",\n",
    "    \"CONSTITUENCY\",\n",
    "    \"COUNTY\",\n",
    "    \"ADDRESS\",\n",
    "    \"LOCAL_AUTHORITY_LABEL\",\n",
    "    \"CONSTITUENCY_LABEL\",\n",
    "    \"POSTTOWN\",\n",
    "    \"UPRN\",\n",
    "    \"UPRN_SOURCE\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Efficiency and consumption columns - possibly uses the current energy rating (the target variable) to estimate these? If so, they wouldn't be known at prediction time as they rely on \"future knowledge\"\n",
    "efficiency_columns = [\n",
    "    \"CURRENT_ENERGY_EFFICIENCY\",\n",
    "    \"ENVIRONMENT_IMPACT_CURRENT\",\n",
    "    \"ENERGY_CONSUMPTION_CURRENT\",\n",
    "    \"CO2_EMISSIONS_CURRENT\",\n",
    "    \"CO2_EMISS_CURR_PER_FLOOR_AREA\",\n",
    "    \"LIGHTING_COST_CURRENT\",\n",
    "    \"HEATING_COST_CURRENT\",\n",
    "    \"HOT_WATER_COST_CURRENT\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Columns with _POTENTIAL in them - presumably based on current data that won't be known at the time of prediction, e.g. 'POTENTIAL_ENERGY_RATING' and 'ENVIRONMENT_IMPACT_POTENTIAL'\n",
    "potential_columns = list(data.columns[data.columns.str.contains(\"POTENTIAL\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Date columns (except relating to construction dates)\n",
    "date_columns = [\"INSPECTION_DATE\", \"LODGEMENT_DATE\", \"LODGEMENT_DATETIME\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Other columns that I don't think will be relevant at this stage\n",
    "other_columns = [\"TRANSACTION_TYPE\", \"TENURE\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_drop = (\n",
    "    location_columns\n",
    "    + potential_columns\n",
    "    + efficiency_columns\n",
    "    + date_columns\n",
    "    + other_columns\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.drop(columns=columns_to_drop)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train/test splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into train and holdout test sets for preprocessing and modelling (perform all feature decisions on train only to avoid data leakage)\n",
    "train, test = train_test_split(\n",
    "    data, test_size=0.2, stratify=data[\"CURRENT_ENERGY_RATING\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Train dataset rows: \", len(train))\n",
    "print(\"Test dataset rows: \", len(test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Windows\n",
    "\n",
    "There are descriptions and efficiency columns for many parts of the buildings, e.g. 'WINDOWS_DESCRIPTION', 'WINDOWS_ENERGY_EFF', 'WINDOWS_ENV_EFF'. I will use the description columns only as I think the others may be based on the energy rating itself"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Windows data\n",
    "window_cols = [\n",
    "    \"GLAZED_TYPE\",\n",
    "    \"GLAZED_AREA\",\n",
    "    \"MULTI_GLAZE_PROPORTION\",\n",
    "    \"WINDOWS_DESCRIPTION\",\n",
    "    \"WINDOWS_ENERGY_EFF\",\n",
    "    \"WINDOWS_ENV_EFF\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a table grouped by the target variable to see which columns give most info\n",
    "tb1 = TableOne(\n",
    "    train,\n",
    "    columns=window_cols + [\"CURRENT_ENERGY_RATING\"],\n",
    "    categorical=[\n",
    "        \"GLAZED_TYPE\",\n",
    "        \"GLAZED_AREA\",\n",
    "        \"WINDOWS_DESCRIPTION\",\n",
    "        \"WINDOWS_ENERGY_EFF\",\n",
    "        \"WINDOWS_ENV_EFF\",\n",
    "    ]\n",
    "    + [\"CURRENT_ENERGY_RATING\"],\n",
    "    groupby=\"CURRENT_ENERGY_RATING\",\n",
    ")\n",
    "print(tb1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Most informative windows column is the MULTI_GLAZE_PROPORTION, which is also numeric so I'll drop the rest\n",
    "window_cols_to_drop = [\n",
    "    \"GLAZED_TYPE\",\n",
    "    \"GLAZED_AREA\",\n",
    "    \"WINDOWS_DESCRIPTION\",\n",
    "    \"WINDOWS_ENERGY_EFF\",\n",
    "    \"WINDOWS_ENV_EFF\",\n",
    "]\n",
    "train = train.drop(columns=window_cols_to_drop)\n",
    "test = test.drop(columns=window_cols_to_drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_description_column(\n",
    "    *, df, description_strings, description_col, new_col, missing_val\n",
    "):\n",
    "    \"\"\"Convert a column containing text descriptions to specified categories.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): dataframe containing description_col.\n",
    "        description_strings (list): substrings to search for in description_col.\n",
    "        description_col (str): column to be converted.\n",
    "        new_col (str): name of new column.\n",
    "        missing_val (str or float): value to use for infilling missing values.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: input df with new_col column.\n",
    "    \"\"\"\n",
    "    df[description_col] = df[description_col].fillna(missing_val)\n",
    "    for description in description_strings:\n",
    "        df.loc[df[description_col].str.contains(description), new_col] = description\n",
    "    df[new_col] = df[new_col].fillna(missing_val)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Walls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wall_columns = list(train.columns[train.columns.str.contains(\"WALLS\")])\n",
    "\n",
    "# Create a few types of wall types based on looking at those present in the WALLS_DESCRIPTION column using value_counts()\n",
    "wall_types = [\n",
    "    \"Solid brick\",\n",
    "    \"Cavity wall\",\n",
    "    \"Stone\",\n",
    "    \"Cob\",\n",
    "    \"System built\",\n",
    "    \"Timber frame\",\n",
    "]\n",
    "\n",
    "\n",
    "def preprocess_walls_data(*, df, wall_types, wall_columns):\n",
    "    df = convert_description_column(\n",
    "        df=df,\n",
    "        description_col=\"WALLS_DESCRIPTION\",\n",
    "        new_col=\"WALL_TYPE\",\n",
    "        description_strings=wall_types,\n",
    "        missing_val=\"Other\",\n",
    "    )\n",
    "\n",
    "    # Also search for wall insulation in the description and pull out the main types\n",
    "    df = convert_description_column(\n",
    "        df=df,\n",
    "        description_col=\"WALLS_DESCRIPTION\",\n",
    "        new_col=\"WALL_INSULATION\",\n",
    "        description_strings=[\n",
    "            \"no insulation\",\n",
    "            \"with internal insulation\",\n",
    "            \"with external insulation\",\n",
    "            \"partial insulation\",\n",
    "            \"insulated\",\n",
    "        ],\n",
    "        missing_val=\"no insulation\",\n",
    "    )\n",
    "\n",
    "    # Create a binary feature for wall insulation\n",
    "    df[\"WALL_INSULATION\"] = np.where(df[\"WALL_INSULATION\"] == \"no insulation\", 0, 1)\n",
    "    df = df.drop(columns=wall_columns)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = preprocess_walls_data(\n",
    "    df=train, wall_types=wall_types, wall_columns=wall_columns\n",
    ")\n",
    "test = preprocess_walls_data(df=test, wall_types=wall_types, wall_columns=wall_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Floors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Similar above but for floors\n",
    "floor_columns = list(train.columns[train.columns.str.contains(\"FLOOR\")])\n",
    "floor_types = [\"Solid\", \"Suspended\", \"Other property below\"]\n",
    "\n",
    "\n",
    "def preprocess_floor_data(*, df, floor_types):\n",
    "    \"\"\"Preprocess floor data by binning descriptions and creating an insulation feature.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): dataframe containing FLOOR_DESCRIPTION column.\n",
    "        floor_types (list): floor types of interest (to be searched for in description).\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: input dataframe with new FLOOR_TYPE and FLOOR_INSULATION columns.\n",
    "    \"\"\"\n",
    "    df[\"FLOOR_DESCRIPTION\"] = df[\"FLOOR_DESCRIPTION\"].replace(\n",
    "        {\n",
    "            \"(another dwelling below)\": \"Other property below\",\n",
    "            \"(other premises below)\": \"Other property below\",\n",
    "        }\n",
    "    )\n",
    "\n",
    "    df = convert_description_column(\n",
    "        df=df,\n",
    "        description_col=\"FLOOR_DESCRIPTION\",\n",
    "        new_col=\"FLOOR_TYPE\",\n",
    "        description_strings=floor_types,\n",
    "        missing_val=\"Other\",\n",
    "    )\n",
    "\n",
    "    df = convert_description_column(\n",
    "        df=df,\n",
    "        description_col=\"FLOOR_DESCRIPTION\",\n",
    "        new_col=\"FLOOR_INSULATION\",\n",
    "        description_strings=[\"no insulation\", \"insulated\", \"limited insulation\"],\n",
    "        missing_val=\"no insulation\",\n",
    "    )\n",
    "\n",
    "    df[\"FLOOR_INSULATION\"] = np.where(df[\"FLOOR_INSULATION\"] == \"no insulation\", 0, 1)\n",
    "\n",
    "    df.loc[df[\"FLOOR_TYPE\"] == \"Other property below\", \"FLOOR_INSULATION\"] = 1\n",
    "\n",
    "    df = df.drop(columns=[\"FLOOR_DESCRIPTION\", \"FLOOR_ENERGY_EFF\", \"FLOOR_ENV_EFF\"])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = preprocess_floor_data(df=train, floor_types=floor_types)\n",
    "test = preprocess_floor_data(df=test, floor_types=floor_types)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Roof"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "roof_columns = list(train.columns[train.columns.str.contains(\"ROOF\")])\n",
    "roof_types = [\"Pitched\", \"Flat\", \"Other property above\"]\n",
    "\n",
    "\n",
    "def preprocess_roof_data(*, df, roof_types, roof_columns):\n",
    "    \"\"\"Preprocess roof data by binning descriptions and creating an insulation feature.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): dataframe containing ROOF_DESCRIPTION column.\n",
    "        roof_types (list): roof types of interest (to be searched for in description).\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: input dataframe with new ROOF_TYPE and FLOOR_INSULATION columns.\n",
    "    \"\"\"\n",
    "    df[\"ROOF_DESCRIPTION\"] = df[\"ROOF_DESCRIPTION\"].replace(\n",
    "        {\n",
    "            \"(another dwelling above)\": \"Other property above\",\n",
    "            \"(other premises above)\": \"Other property above\",\n",
    "        }\n",
    "    )\n",
    "\n",
    "    df = convert_description_column(\n",
    "        df=df,\n",
    "        description_col=\"ROOF_DESCRIPTION\",\n",
    "        new_col=\"ROOF_TYPE\",\n",
    "        description_strings=roof_types,\n",
    "        missing_val=\"Other\",\n",
    "    )\n",
    "    df = convert_description_column(\n",
    "        df=df,\n",
    "        description_col=\"ROOF_DESCRIPTION\",\n",
    "        new_col=\"ROOF_INSULATION\",\n",
    "        description_strings=[\"no insulation\", \"loft insulation\", \"limited insulation\"],\n",
    "        missing_val=\"no insulation\",\n",
    "    )\n",
    "\n",
    "    # Create binary column for roof insulation\n",
    "    df[\"ROOF_INSULATION\"] = np.where(df[\"ROOF_INSULATION\"] == \"no insulation\", 0, 1)\n",
    "    # Set properties with other above them as insulated\n",
    "    df.loc[df[\"ROOF_TYPE\"] == \"Other property above\", \"ROOF_INSULATION\"] = 1\n",
    "\n",
    "    df = df.drop(columns=roof_columns)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = preprocess_roof_data(df=train, roof_columns=roof_columns, roof_types=roof_types)\n",
    "test = preprocess_roof_data(df=test, roof_columns=roof_columns, roof_types=roof_types)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I will drop some further columns here due to time constraints. The hot water and main heating columns overlap\n",
    "# with main fuel, and the lighting columns are partly captured by LOW_ENERGY_LIGHTING\n",
    "\n",
    "columns_to_drop = [\n",
    "    \"HOTWATER_DESCRIPTION\",\n",
    "    \"HOT_WATER_ENERGY_EFF\",\n",
    "    \"HOT_WATER_ENV_EFF\",\n",
    "    \"SECONDHEAT_DESCRIPTION\",\n",
    "    \"SHEATING_ENERGY_EFF\",\n",
    "    \"SHEATING_ENV_EFF\",\n",
    "    \"MAINHEAT_DESCRIPTION\",\n",
    "    \"MAINHEAT_ENERGY_EFF\",\n",
    "    \"MAINHEAT_ENV_EFF\",\n",
    "    \"MAINHEATCONT_DESCRIPTION\",\n",
    "    \"MAINHEATC_ENERGY_EFF\",\n",
    "    \"MAINHEATC_ENV_EFF\",\n",
    "    \"MAIN_HEATING_CONTROLS\",\n",
    "    \"LIGHTING_DESCRIPTION\",\n",
    "    \"LIGHTING_ENERGY_EFF\",\n",
    "    \"LIGHTING_ENV_EFF\",\n",
    "    \"HEAT_LOSS_CORRIDOR\",\n",
    "    \"UNHEATED_CORRIDOR_LENGTH\",\n",
    "    \"MECHANICAL_VENTILATION\",\n",
    "    \"ENERGY_TARIFF\",\n",
    "    \"FLOOR_LEVEL\",\n",
    "]\n",
    "\n",
    "train = train.drop(columns=columns_to_drop)\n",
    "test = test.drop(columns=columns_to_drop)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main fuel source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a feature for the main fuel source\n",
    "train = convert_description_column(\n",
    "    df=train,\n",
    "    description_col=\"MAIN_FUEL\",\n",
    "    new_col=\"MAIN_FUEL_TYPE\",\n",
    "    description_strings=[\"mains gas\", \"electricity\"],\n",
    "    missing_val=\"Other\",\n",
    ")\n",
    "train = train.drop(columns=[\"MAIN_FUEL\"])\n",
    "\n",
    "test = convert_description_column(\n",
    "    df=test,\n",
    "    description_col=\"MAIN_FUEL\",\n",
    "    new_col=\"MAIN_FUEL_TYPE\",\n",
    "    description_strings=[\"mains gas\", \"electricity\"],\n",
    "    missing_val=\"Other\",\n",
    ")\n",
    "test = test.drop(columns=[\"MAIN_FUEL\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Construction age"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess the 'CONSTRUCTION_AGE_BAND' column\n",
    "\n",
    "def preprocess_construction_age(df):\n",
    "    \"\"\"Preprocess construction age column by tidying nans and date ranges/year values.\n",
    "    \n",
    "    NB The column contains a mixture of age bands preceded by the string 'England and Wales: ' and some integer year values. Ideally I'd get the start and end dates for each bands, then map the numeric values into the bins. However I'm using a shortcut to save time (not very robust...)\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): dataframe containing CONSTRUCTION_AGE_BAND column.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: input dataframe with tidied up CONSTRUCTION_AGE_BAND column.\n",
    "    \"\"\"\n",
    "    df[\"CONSTRUCTION_AGE_BAND\"] = df[\"CONSTRUCTION_AGE_BAND\"].replace(\n",
    "        {\"NO DATA!\": np.nan, \"INVALID!\": np.nan}\n",
    "    )\n",
    "\n",
    "    # Only two of the numeric values are pre-2012, so I'll bucket those manually and then add the rest to the 2012 onwards bin\n",
    "    df[\"CONSTRUCTION_AGE_BAND\"] = df[\"CONSTRUCTION_AGE_BAND\"].replace(\n",
    "        {\"1920\": \"England and Wales: 1900-1929\", \"1960\": \"England and Wales: 1950-1966\"}\n",
    "    )\n",
    "\n",
    "    # Create a new column with the existing date bins and all year values added to the latest\n",
    "    df[\"CONSTRUCTION_AGE_BAND\"] = [\n",
    "        \"England and Wales: 2012 onwards\" if type(x) is not float and len(x) == 4 else x\n",
    "        for x in df[\"CONSTRUCTION_AGE_BAND\"]\n",
    "    ]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = preprocess_construction_age(train)\n",
    "test = preprocess_construction_age(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train[\"CONSTRUCTION_AGE_BAND\"].value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test[\"CONSTRUCTION_AGE_BAND\"].value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_string_to_binary(df, column):\n",
    "    \"\"\"Convert string Y/N values to binary 1/0 values.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): dataframe containing column with Y/N values.\n",
    "        column (str): name of column to process in df.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: input dataframe with converted Y/N values in specified column.\n",
    "    \"\"\"\n",
    "    df[column] = df[column].replace({\"Y\": 1, \"N\": 0})\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tidy up missing data and convert Y/N strings to binary\n",
    "def tidy_up(df):\n",
    "    \"\"\"Final tidy up of data and calls to convert_string_to_binary function.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): dataframe containing BUILT_FORM, MAINS_GAS_FLAG, FLAT_TOP_STOREY and SOLAR_WATER_HEATING_FLAG columns.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: input dataframe with preprocessing applied.\n",
    "    \"\"\"\n",
    "    df[\"BUILT_FORM\"] = df[\"BUILT_FORM\"].replace({\"NO DATA!\": np.nan})\n",
    "\n",
    "    df = convert_string_to_binary(df=df, column=\"MAINS_GAS_FLAG\")\n",
    "    df = convert_string_to_binary(df=df, column=\"FLAT_TOP_STOREY\")\n",
    "    df = convert_string_to_binary(df=df, column=\"SOLAR_WATER_HEATING_FLAG\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = tidy_up(train)\n",
    "test = tidy_up(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train[\"PROPERTY_TYPE\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new profiling report on the processed train data\n",
    "profile = ProfileReport(train)\n",
    "profile.to_file(\"train_dataset.html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode categorical variables using one hot encoding\n",
    "# This creates a new column (binary) for each option in the column, not good for high cardinality data. Another option is target encoding\n",
    "categorical_variables = [\n",
    "    \"PROPERTY_TYPE\",\n",
    "    \"BUILT_FORM\",\n",
    "    \"CONSTRUCTION_AGE_BAND\",\n",
    "    \"WALL_TYPE\",\n",
    "    \"FLOOR_TYPE\",\n",
    "    \"ROOF_TYPE\",\n",
    "    \"MAIN_FUEL_TYPE\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.get_dummies(train, columns=categorical_variables, dummy_na=True)\n",
    "test = pd.get_dummies(test, columns=categorical_variables, dummy_na=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Target variable encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# label encode the target variable (going from A, B, C to 0, 1, 2 etc)\n",
    "le = LabelEncoder()\n",
    "features_train = train.drop(columns=[\"CURRENT_ENERGY_RATING\"])\n",
    "target_train = train[\"CURRENT_ENERGY_RATING\"]\n",
    "\n",
    "target_train = le.fit_transform(target_train)\n",
    "\n",
    "features_test = test.drop(columns=[\"CURRENT_ENERGY_RATING\"])\n",
    "target_test = test[\"CURRENT_ENERGY_RATING\"]\n",
    "# Use the label encoder from train on the holdout test data\n",
    "target_test = le.transform(target_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will use sparsity aware algorithms (nan values allowed), otherwise I would need to impute to fill the missing values. Options for that include median/mode over the train set, K nearest neighbours or MICE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model_cv(X, y, model):\n",
    "    \"\"\"Perform 10-fold cross validation on the train data and report the accuracy score for all folds.\n",
    "\n",
    "    Args:\n",
    "        X (pd.DataFrame): train feature set.\n",
    "        y (pd.Series): train target variable.\n",
    "        model (classifier): a classification model to be used for cross validation.\n",
    "    \"\"\"\n",
    "    # define evaluation procedure\n",
    "    cv = StratifiedKFold(n_splits=10)\n",
    "    # evaluate model\n",
    "    scores = cross_val_score(model, X, y, scoring=\"accuracy\", cv=cv, n_jobs=-1)\n",
    "    print(\"Accuracy scores on CV folds: \", scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a baseline model using a dummy classifier (uses no training data)\n",
    "evaluate_model_cv(\n",
    "    X=features_train, y=target_train, model=DummyClassifier(strategy=\"stratified\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train an xgboost model to compare with the baseline model\n",
    "evaluate_model_cv(X=features_train, y=target_train, model=XGBClassifier(random_state=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The xgboost model significantly outperforms the baseline model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit on all train data, then predict on the holdout test data using xgboost\n",
    "clf = XGBClassifier(random_state=1)\n",
    "clf.fit(features_train, target_train)\n",
    "predictions = clf.predict(features_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and plot a confusion matrix for the model to visualise the predictions and see where the model is correct and incorrect\n",
    "class_names = [\"A\", \"B\", \"C\", \"D\", \"E\", \"F\", \"G\"]\n",
    "cm = confusion_matrix(y_pred=predictions, y_true=target_test)\n",
    "\n",
    "disp = ConfusionMatrixDisplay(cm, display_labels=class_names)\n",
    "disp.plot(cmap=\"Blues\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explainability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use SHAP to explain which features are important to the model's predictions across the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a SHAP explainer for tree based models - takes a few min to run\n",
    "explainer = shap.TreeExplainer(clf)\n",
    "shap_values = explainer.shap_values(features_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the most important model features broken down by class (0=A, 1=B etc)\n",
    "shap.summary_plot(shap_values, features=features_train, class_inds='original', class_names=clf.classes_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  SHAP summary plot showing feature importances for a single class - C rating\n",
    "shap.summary_plot(shap_values[2], features=features_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  SHAP summary plot showing feature importances for a single class - G rating\n",
    "shap.summary_plot(shap_values[6], features=features_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next steps with more time:\n",
    "- feature dropout to remove unimportant columns\n",
    "- try more algorithms in cross validation\n",
    "- include some of the dropped columns\n",
    "- generalise some functions to avoid hard coded column names\n",
    "- add unit tests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mortar",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
